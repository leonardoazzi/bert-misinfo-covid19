{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0C65qU6wfK7"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEIISTjXwfK9"
   },
   "outputs": [],
   "source": [
    "%pip install pandas transformers datasets torch scikit-learn evaluate seaborn imblearn accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ynh4tcMwfK-"
   },
   "source": [
    "# Preparação de dados\n",
    "Carrega o dataset a ser utilizado para fine-tuning e seleciona os atributos mais relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE0cUc2_wfK-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu1pmrPkwfK_"
   },
   "source": [
    "Faz o download do dataset anotado no diretório ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXhFPYN4wfK_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./data/covidbr_labeled.csv'):\n",
    "  !mkdir data\n",
    "  !curl -L -o ./data/covidbr_labeled.csv https://zenodo.org/records/5193932/files/covidbr_labeled.csv\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKPOuVLVwfLA"
   },
   "outputs": [],
   "source": [
    "original_dataset_df = pd.read_csv('./data/covidbr_labeled.csv')\n",
    "original_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9osEfdnwfLA"
   },
   "outputs": [],
   "source": [
    "dataset_df = original_dataset_df[[\"text\", \"misinformation\"]]\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o75C_oFXwfLA"
   },
   "source": [
    "# Análise exploratória de dados\n",
    "\n",
    "O objetivo é entender melhor e sumarizar as características dos dados, analisando quantidade e tipos de atributos, verificando distribuição do atributo alvo, identificando padrões e anomalias, removendo atributos que pareçam irrelevantes ou problemáticos, etc. Utilize gráficos e sumarizações estatísticas para a EDA. Verifique potenciais problemas nos dados, como por exemplo, a necessidade de normalizar os atributos, balancear classes, ou remover instâncias ou atributos por inconsistências nos dados.\n",
    "\n",
    "- P1. Qual a quantidade e tipos de atributos? Existem inconsistências?\n",
    "  - Quais são os atributos disponíveis?\n",
    "  - Existem inconsistências nos atributos? (Atributos vazios, potenciais erros, etc)\n",
    "  - Existem atributos que necessitam ser removidos ou transformados?\n",
    "- P2. Qual a distribuição do atributo alvo?\n",
    "  - Quais são as classes alvo? Qual a distribuição entre as classes? Está balanceada ou desbalanceada?\n",
    "- P3. Quais os padrões e anomalias dos atributos?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SwOgg9g0lmF"
   },
   "source": [
    "## P1. Qual a quantidade e tipos de atributos? Existem inconsistências?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KCO1q4OwfLB"
   },
   "outputs": [],
   "source": [
    "dataset_df.info(verbose = False, memory_usage = False, show_counts = True) # mostra o tipo e a quantidade de itens não nulos de cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDz_81f6zPba"
   },
   "outputs": [],
   "source": [
    "dataset_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9p7PRlU0IPb"
   },
   "source": [
    "## P2. Qual a distribuição do atributo alvo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS1RCHwlzkxb"
   },
   "outputs": [],
   "source": [
    "dataset_df['misinformation'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjVV2NkC3sI8"
   },
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02jH988i4k55"
   },
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dhk4RbXcH0sM"
   },
   "source": [
    "Carrega o tokenizador para `bert-base-portuguese-cased` (BERTimbau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbEOQ9LA4tOs"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD3RvHxcH5Gm"
   },
   "source": [
    "Aplica a tokenização para todas as instâncias de `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4n9udCt5hI9"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    print(examples)\n",
    "    return tokenizer(str(examples), padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = dataset_df.apply(lambda row: tokenize_function(row[\"text\"]), axis=1)\n",
    "\n",
    "# Inspect tokenized samples\n",
    "tokenized_df = pd.DataFrame(tokenized_datasets, columns=[\"tk_text\"])\n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwEuSUs6-zfM"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([tokenized_df, dataset_df[\"misinformation\"]], axis=1, join=\"inner\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fizsZ4a_4jLM"
   },
   "source": [
    "## Balanceamento de classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tpWAkO9REfo"
   },
   "source": [
    "Utilizando o cálculo de class_weights.\n",
    "\n",
    "Fonte: https://medium.com/@heyamit10/fine-tuning-bert-for-classification-a-practical-guide-b8c1c56f252c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-6B04Qi3uqB"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels = data[\"misinformation\"]\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF0zFFlUwfLB"
   },
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY-ilYbTIbyJ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "from transformers import BertModel\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNoZT8mIJMlF"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Keep only the classification head trainable\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dytbZu4PfLu"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Directory for saving model checkpoints\n",
    "    #evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=5e-5,              # Start with a small learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size per GPU\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    save_total_limit=2,              # Limit checkpoints to save space\n",
    "    #load_best_model_at_end=True,     # Automatically load the best checkpoint\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    logging_steps=100,               # Log every 100 steps\n",
    "    fp16=True                        # Enable mixed precision for faster training\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "# prepare cross validation\n",
    "k=5\n",
    "kf = KFold(n_splits=k, random_state=1, shuffle=True)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    train_df = data.iloc[train_index]\n",
    "    val_df = data.iloc[val_index]\n",
    "    \n",
    "    # Convert to Hugging Face Dataset format\n",
    "    train_dataset = train_df.to_dict(orient=\"list\")\n",
    "    val_dataset = val_df.to_dict(orient=\"list\")\n",
    "    \n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: {\"f1\": f1_score.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)[\"f1\"]}\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Fold {fold + 1} F1 Score: {eval_result['eval_f1']}\")\n",
    "    fold_results.append(eval_result['eval_f1'])\n",
    "\n",
    "# Calculate and print the mean F1 score\n",
    "mean_f1 = sum(fold_results) / len(fold_results)\n",
    "print(f\"Mean F1 Score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
