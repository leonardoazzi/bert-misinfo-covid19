{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração de dados: EDA e pré-processamento\n",
    "Giulia Chimini Stefainski, Leonardo Azzi Martins, Matheus de Moraes Costa\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo:** realizar uma análise exploratória de dados, e a partir disto definir possibilidades de pré-processamento para o dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.6.0+cu124 \\\n",
    "  --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==1.5.3 transformers==4.50.2 datasets==3.5.0 scikit-learn==1.4.2 evaluate==0.4.3 seaborn==0.13.2 imblearn accelerate==1.5.2 emoji==2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade numpy transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall --upgrade numpy pandas scikit-learn torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação de dados\n",
    "Carrega o dataset a ser utilizado para fine-tuning e seleciona os atributos mais relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz o download do dataset anotado no diretório ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./data/covidbr_labeled.csv'):\n",
    "  %mkdir data\n",
    "  %curl -L -o ./data/covidbr_labeled.csv https://zenodo.org/records/5193932/files/covidbr_labeled.csv\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_df = pd.read_csv('./data/covidbr_labeled.csv')\n",
    "original_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = original_dataset_df[[\"text\", \"misinformation\"]]\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise exploratória de dados\n",
    "\n",
    "O objetivo é entender melhor e sumarizar as características dos dados, analisando quantidade e tipos de atributos, verificando distribuição do atributo alvo, identificando padrões e anomalias, removendo atributos que pareçam irrelevantes ou problemáticos, etc. Utilize gráficos e sumarizações estatísticas para a EDA. Verifique potenciais problemas nos dados, como por exemplo, a necessidade de normalizar os atributos, balancear classes, ou remover instâncias ou atributos por inconsistências nos dados.\n",
    "\n",
    "- P1. Qual a quantidade e tipos de atributos? Existem inconsistências?\n",
    "  - Quais são os atributos disponíveis?\n",
    "  - Existem inconsistências nos atributos? (Atributos vazios, potenciais erros, etc)\n",
    "  - Existem atributos que necessitam ser removidos ou transformados?\n",
    "- P2. Existem padrões e anomalias nos dados?\n",
    "  - Existem tendências ou inconsistências nos atributos? (Atributos vazios, potenciais erros, etc)\n",
    "- P3. Qual a distribuição do atributo alvo?\n",
    "  - Quais são as classes alvo? Qual a distribuição entre as classes? Está balanceada ou desbalanceada?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1. Qual a quantidade e tipos de atributos? Existem inconsistências?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Quais são os atributos disponíveis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2. Existem padrões e anomalias nos dados?\n",
    "Existem tendências ou inconsistências nos atributos? (Atributos vazios, potenciais erros, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Quais são os padrões?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Tokeniza o atributo 'text' de dataset_df por classe de misinformation usando whitespaces\n",
    "for label in sorted(dataset_df['misinformation'].unique()):\n",
    "    print(f\"\\nClasse misinformation = {label}\")\n",
    "    texts = dataset_df[dataset_df['misinformation'] == label]['text']\n",
    "    tokenized = texts.apply(lambda x: str(x).split())\n",
    "    token_lengths = tokenized.apply(len)\n",
    "    print(token_lengths.describe())\n",
    "    print(f\"Mediana do comprimento dos tokens: {token_lengths.median()}\")\n",
    "\n",
    "    words = texts.apply(lambda x: str(x).split())\n",
    "    word_lengths = words.apply(lambda ws: [len(w) for w in ws if len(w) > 0])\n",
    "    all_word_lengths = [l for sublist in word_lengths for l in sublist]\n",
    "    avg_word_length = np.mean(all_word_lengths) if all_word_lengths else 0\n",
    "    print(f\"Tamanho médio de palavras (em caracteres) para classe {label}: {avg_word_length:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=50, edgecolor='black', log=True)\n",
    "plt.title('Distribuição do número de tokens do atributo text (escala log)')\n",
    "plt.xlabel('Número de tokens')\n",
    "plt.ylabel('Frequência (escala log)')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot do número de tokens por texto, separado por classe de misinformation\n",
    "plt.figure(figsize=(10, 6))\n",
    "token_lengths_by_class = [\n",
    "    dataset_df[dataset_df['misinformation'] == label]['text'].apply(lambda x: len(str(x).split()))\n",
    "    for label in sorted(dataset_df['misinformation'].unique())\n",
    "]\n",
    "plt.boxplot(token_lengths_by_class, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='white', color='black', linewidth=1.5),\n",
    "            medianprops=dict(color='red', linewidth=2),\n",
    "            whiskerprops=dict(color='black', linewidth=1.5),\n",
    "            capprops=dict(color='black', linewidth=1.5),\n",
    "            flierprops=dict(marker='o', markerfacecolor='gray', markersize=4, alpha=0.4, markeredgecolor='black'),\n",
    "            widths=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('Box-plot do número de tokens por texto para cada classe (escala log)')\n",
    "plt.xlabel('Classe de misinformation', fontsize=14)\n",
    "plt.ylabel('Número de tokens (escala log)', fontsize=14)\n",
    "plt.xticks([1, 2], sorted(dataset_df['misinformation'].unique()))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6, which='both')\n",
    "plt.gca().yaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Existem inconsistências nos atributos? (Atributos vazios, potenciais erros, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[dataset_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove instância com texto nulo, pois é irrelevante para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.dropna()\n",
    "dataset_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# Busca por textos que contém qualquer ocorrência de URLs\n",
    "any_url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+|\\b[^\\s]+?\\.(com|br|org|net|gov|edu|pt)\\b)'\n",
    "\n",
    "# Busca por textos que começam com URLs\n",
    "start_url_pattern = r'^(https?://[^\\s]+|www\\.[^\\s]+|\\b[^\\s]+?\\.(com|br|org|net|gov|edu|pt)\\b)'\n",
    "\n",
    "# Busca por textos que contém exclusivamente URLs\n",
    "only_url_pattern = r'^(https?://[^\\s]+|www\\.[^\\s]+|\\b[^\\s]+?\\.(com|br|org|net|gov|edu|pt))$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Busca por textos que começam com URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busca instâncias de text onde começa com uma URL. Conforme Martins et al. 2021, estas instâncias podem dificultar a classificação, resultando em um ganho de aprox. 10% em F1-score ao remover estas instâncias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url_df = dataset_df[dataset_df['text'].str.contains(start_url_pattern, na=False)]\n",
    "start_url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Busca por textos que contém qualquer ocorrência de URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = dataset_df.copy()\n",
    "url_df = url_df[url_df['text'].str.contains(any_url_pattern, regex=True, flags=re.IGNORECASE)]\n",
    "url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df[url_df['misinformation'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df[url_df['misinformation'] == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Busca por textos que **não** contém ocorrências de URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_url_df = dataset_df.copy()\n",
    "no_url_df = no_url_df[~no_url_df['text'].str.contains(any_url_pattern, regex=True, flags=re.IGNORECASE)]\n",
    "no_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_url_df[no_url_df['misinformation'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_url_df[no_url_df['misinformation'] == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Busca por textos que contém **exclusivamente** URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_url_rows = dataset_df[dataset_df['text'].str.match(only_url_pattern, na=False)]\n",
    "only_url_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_url_rows[only_url_rows['misinformation'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_url_rows[only_url_rows['misinformation'] == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Existe o mesmo dataset filtrado conforme Martins et al. (2021)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduz o notebook de Martins et al. (2021), que reporta existirem 1.509 mensagens com apenas URLs como conteúdo em texto, divergindo do nosso achado de 498 instâncias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "martins_df = dataset_df.copy()\n",
    "martins_df['cleanLinks'] = martins_df['text'].apply(lambda x: re.split(r'http:\\/\\/.*', str(x))[0])\n",
    "martins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martins_df['cleanLinks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martins_df[martins_df['cleanLinks'] != '' ].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metodologia aplicada não foi capaz de filtrar corretamente as mensagens exclusivamente compostas por URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "labels = ['Contém URL', 'Sem URL', 'Apenas URL']\n",
    "\n",
    "tipo = []\n",
    "classe = []\n",
    "for df, nome in zip([url_df, no_url_df, only_url_rows], labels):\n",
    "    tipo.extend([nome] * len(df))\n",
    "    # Use the correct column name for the class label\n",
    "    if 'labels' in df.columns:\n",
    "        classe.extend(df['labels'].tolist())\n",
    "    else:\n",
    "        classe.extend(df['misinformation'].tolist())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=tipo, hue=classe, palette=['#377eb8', '#e41a1c'])\n",
    "plt.title('Distribuição das classes por tipo de mensagem')\n",
    "plt.xlabel('Tipo de mensagem')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.legend(title='Classe', labels=['Não desinformação', 'Desinformação'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Emojis 🤠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Busca mensagens compostas por emojis\n",
    "- `emoji_count`: conta a quantidade de emojis em 'text'\n",
    "- `emoji_ratio`: calcula a taxa de emojis por mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_emojis(text):\n",
    "    return sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "dataset_df['emoji_count'] = dataset_df['text'].apply(count_emojis)\n",
    "dataset_df['char_count'] = dataset_df['text'].apply(char_count)\n",
    "dataset_df['word_count'] = dataset_df['text'].apply(word_count)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    return count_emojis(text) / char_count(text) if char_count(text) > 0 else 0\n",
    "\n",
    "dataset_df['emoji_ratio'] = dataset_df['text'].apply(emoji_ratio)\n",
    "\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contagem de emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset_df.sort_values(by='emoji_count', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['emoji_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[dataset_df['misinformation'] == 0]['emoji_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[dataset_df['misinformation'] == 1]['emoji_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='misinformation', y='emoji_count', data=dataset_df, palette=['#377eb8', '#e41a1c'])\n",
    "plt.title('Box-plot do número de emojis (escala log)')\n",
    "plt.xlabel('Desinformação')\n",
    "plt.ylabel('Número de emojis (log)')\n",
    "plt.yscale('log')   \n",
    "plt.gca().yaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "\n",
    "plt.legend(title='Classe', labels=['Não desinformação', 'Desinformação'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dataset_df['emoji_count'], edgecolor='black', bins=range(0, int(dataset_df['emoji_count'].max()) + 25, 25), alpha=0.7)\n",
    "plt.title('Histograma do número de emojis por mensagem (escala log)')\n",
    "plt.xlabel('Número de emojis por mensagem')\n",
    "plt.ylabel('Frequência (log)')\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(0, int(dataset_df['emoji_count'].max()) + 1, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taxa de emojis por mensagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica as instâncias com maior emoji_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset_df.sort_values(by='emoji_ratio', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['emoji_ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe alguma relação entre a taxa de emojis e o atributo preditivo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[dataset_df['misinformation'] == 0]['emoji_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[dataset_df['misinformation'] == 1]['emoji_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='misinformation', y='emoji_ratio', data=dataset_df, palette=['#377eb8', '#e41a1c'])\n",
    "plt.title('Box-plot da taxa de emojis por caractere da mensagem (escala log)')\n",
    "plt.xlabel('Desinformação')\n",
    "plt.ylabel('Taxa de emojis por caractere (log)')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "plt.legend(title='Classe', labels=['Não desinformação', 'Desinformação'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica as instâncias com maior emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset_df.sort_values(by='emoji_count', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não existem textos com emoji ratio maior que ~0.02. Portanto, não precisam ser tratados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dataset_df['emoji_ratio'], edgecolor='black')\n",
    "plt.title('Histograma da taxa de emojis por caractere (escala log)')\n",
    "plt.xlabel('Taxa de emojis por caractere')\n",
    "plt.ylabel('Frequência (log)')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2. Qual a distribuição do atributo alvo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "series = dataset_df['misinformation'].value_counts()\n",
    "print(series)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=dataset_df['misinformation'],\n",
    "    data=dataset_df,\n",
    "    hue='misinformation',\n",
    "    palette=['#377eb8', '#e41a1c'],\n",
    "    order=dataset_df['misinformation'].value_counts().index\n",
    ")\n",
    "plt.legend(title='Classe', labels=['Não desinformação', 'Desinformação'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isto indica que o dataset está desbalanceado, fator que pode enviesar o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E qual a distribuição removendo instâncias que contém URLs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "series = no_url_df['misinformation'].value_counts()\n",
    "\n",
    "print(series)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "sns.countplot(x=no_url_df['misinformation'], data = no_url_df,\n",
    "              hue='misinformation', palette=['#377eb8', '#e41a1c'],\n",
    "              order=dataset_df['misinformation'].value_counts().index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações iniciais\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando o dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset removendo instâncias onde o texto contém URLs em seu início"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_start_url_df = dataset_df[~dataset_df['text'].str.contains(r'^(http|www)', na=False)].reset_index(drop=True)\n",
    "no_start_url_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset removendo todas as URLs do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_url_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "O HuggingFace Trainer utiliza o rótulo labels para identificar os rótulos no treinamento. Renomeando a coluna alvo para 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.rename(columns={'misinformation': 'labels'})\n",
    "no_start_url_df = no_start_url_df.rename(columns={'misinformation': 'labels'})\n",
    "no_url_df = no_url_df.rename(columns={'misinformation': 'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega o tokenizador para `bert-base-portuguese-cased` (BERTimbau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "\n",
    "hf_model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos uma função de tokenização, que será utilizada para tokenizar cada valor de um Pandas DataFrame em forma de função de mapeamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  dataset_tk = dataset.map(tokenize_function, batched=True, remove_columns=['text']) #'__index_level_0__'\n",
    "  return dataset_tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento de classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que o dataset tem sua classe misinformation desbalanceada, utilizou-se o método de cálculo de class_weights, que atribui pesos na função loss do treinador para 'compensar' o desbalanceamento.\n",
    "\n",
    "\"If \"balanced\", class weights will be given by `n_samples / (n_classes * np.bincount(y=labels))`. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.\"\n",
    "\n",
    "Referências:\n",
    "- https://medium.com/@heyamit10/fine-tuning-bert-for-classification-a-practical-guide-b8c1c56f252c\n",
    "- https://discuss.huggingface.co/t/class-weights-for-bertforsequenceclassification/1674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_class_weights(df):\n",
    "- Cria uma instância do CrossEntropyLoss com os pesos calculados\n",
    "- Recria a classe WeightedTrainer para 'sobrescrever' a classe original no HuggingFace Trainer, utilizada a computação do loss ponderada configurada acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(df):\n",
    "  labels = df[\"labels\"]\n",
    "\n",
    "  class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "\n",
    "  class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "  print(class_weights)\n",
    "\n",
    "  loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "  class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "      \n",
    "  return WeightedTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "get_class_weights(dataset_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
